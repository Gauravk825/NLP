{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: joblib in d:\\pg-dai\\environment\\env1\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.5.15-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/42.0 kB ? eta -:--:--\n",
      "     -------------------------------------- 42.0/42.0 kB 503.7 kB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in d:\\pg-dai\\environment\\env1\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in d:\\pg-dai\\environment\\env1\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.1/1.5 MB 2.2 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.4/1.5 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.4/1.5 MB 2.8 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.5/1.5 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.5/1.5 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.6/1.5 MB 2.5 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.7/1.5 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 0.8/1.5 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 0.9/1.5 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 0.9/1.5 MB 2.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.2/1.5 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.4/1.5 MB 2.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.5 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 2.1 MB/s eta 0:00:00\n",
      "Downloading regex-2024.5.15-cp311-cp311-win_amd64.whl (268 kB)\n",
      "   ---------------------------------------- 0.0/269.0 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 61.4/269.0 kB 1.7 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 61.4/269.0 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 112.6/269.0 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 174.1/269.0 kB 1.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 225.3/269.0 kB 1.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 225.3/269.0 kB 1.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 225.3/269.0 kB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- 269.0/269.0 kB 788.8 kB/s eta 0:00:00\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "   ---------------------------------------- 0.0/97.9 kB ? eta -:--:--\n",
      "   --------------------------------- ------ 81.9/97.9 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 97.9/97.9 kB 1.4 MB/s eta 0:00:00\n",
      "Installing collected packages: regex, click, nltk\n",
      "Successfully installed click-8.1.7 nltk-3.8.1 regex-2024.5.15\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\GAURAV\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\GAURAV\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\GAURAV\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\GAURAV\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\GAURAV\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package indian to\n",
      "[nltk_data]     C:\\Users\\GAURAV\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\indian.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\GAURAV\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\") # Tokenization \n",
    "nltk.download('stopwords') # stop removal\n",
    "nltk.download('averaged_perceptron_tagger') # POS Tagging\n",
    "nltk.download('wordnet') # wordnet datsbase and lemmatization \n",
    "nltk.download('omw-1.4') #stemming\n",
    "nltk.download('indian') # Indian language POS tagging\n",
    "nltk.download('maxent_ne_chunker') # Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'They told that their ages are 25 27 and 31 resdpectively'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the average of ages mentioned in the above snetance\n",
    "\n",
    "intput = 'They told that their ages are 25 27 and 31 resdpectively'\n",
    "lst = intput.split()\n",
    "s = 0\n",
    "ages = []\n",
    "for word in lst:\n",
    "    if word.isdigit():\n",
    "        ages.append(int(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25, 27, 31]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.666666666666668"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(ages)/ len(ages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.666666666666668"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([int(word) for word in sent.split() if word.isdigit()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toeknization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'Hello friends! How are you? Welcome to Python Progrmming.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the function s\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello friends!', 'How are you?', 'Welcome to Python Progrmming.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Segmentation \n",
    "sent_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'Python',\n",
       " 'Progrmming',\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word = word_tokenize(sent)\n",
    "# #print(word)\n",
    "\n",
    "# count = 0\n",
    "# for word in word:\n",
    "#     if word.isalnum():\n",
    "#         print(word)\n",
    "#         count = count + 1\n",
    "\n",
    "# print(count)\n",
    "# # per = (coun/len(word))/100\n",
    "\n",
    "# print(per)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pun_count = len([word for word in word_tokenize(sent) if not word.isalnum()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pun_count / len(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof('=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function getsizeof in module sys:\n",
      "\n",
      "getsizeof(...)\n",
      "    getsizeof(object [, default]) -> int\n",
      "    \n",
      "    Return the size of object in bytes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sys.getsizeof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function chr(i, /)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "char = '\\u0935'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "à¤µ\n"
     ]
    }
   ],
   "source": [
    "print(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "char = '\\u0935\\u0940'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "à¤µà¥€\n"
     ]
    }
   ],
   "source": [
    "print(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2357"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('à¤µ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à¤µ'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(2357)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à¤µ'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0x935)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof('à¤µ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'à¤—à¥Œà¤°à¤µ à¤•à¥à¤®à¤¾à¤° à¤¸à¤¿à¤‚à¤¹'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name.startswith('à¤—')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Space Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('mydata.txt') as file:\n",
    "#     data = file.readable()\n",
    "#     for line in file:\n",
    "#         print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('mydata.txt') as file:\n",
    "#     data = file.readline()\n",
    "#     print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('mydata.txt') as file:\n",
    "#     data = file.readlines()\n",
    "#     print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends!ðŸ˜€ How are you?ðŸ‘‹\n",
      "WelcomeðŸ™ tothe worldðŸŒŽ of python Programming.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('mydata.txt') as file:\n",
    "    data = file.read()\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Friends!ðŸ˜€ How are you?ðŸ‘‹\\nWelcomeðŸ™ tothe worldðŸŒŽ of python Programming.\\n\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends!ðŸ˜€',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you?ðŸ‘‹\\nWelcomeðŸ™',\n",
       " 'tothe',\n",
       " 'worldðŸŒŽ',\n",
       " 'of',\n",
       " 'python',\n",
       " 'Programming.\\n\\n']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the class \n",
    "from nltk.tokenize import SpaceTokenizer\n",
    "\n",
    "# Create the object\n",
    "tk = SpaceTokenizer()\n",
    "\n",
    "# Tokenize the data\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tab Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mydata.txt') as file:\n",
    "    data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open('mydata.txt')\n",
    "# data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Friends!ðŸ˜€ How are you?ðŸ‘‹\\nWelcomeðŸ™ tothe worldðŸŒŽ of python Programming.\\n\\n']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the class \n",
    "from nltk.tokenize import TabTokenizer\n",
    "tk = TabTokenizer()\n",
    "\n",
    "# Tokenize the data\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Friends!ðŸ˜€ How are you?ðŸ‘‹',\n",
       " 'WelcomeðŸ™ tothe worldðŸŒŽ of python Programming.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the class \n",
    "from nltk.tokenize import LineTokenizer\n",
    "tk = LineTokenizer()\n",
    "\n",
    "# Tokenize the data\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### White space Tokeniazer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends!ðŸ˜€',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you?ðŸ‘‹',\n",
       " 'WelcomeðŸ™',\n",
       " 'tothe',\n",
       " 'worldðŸŒŽ',\n",
       " 'of',\n",
       " 'python',\n",
       " 'Programming.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the class \n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "tk = WhitespaceTokenizer()\n",
    "\n",
    "# Tokenize the data\n",
    "tk.tokenize(data) # It wil also combine newline and tab along with white space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MWE Tokenizer ( Multi word expresion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = '''The Van Rossum is Python createor , visiting Pune this week. The devlopment\n",
    "community is very eager to meet Van Rossum'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Van Rossum is Python createor , visiting Pune this week. The devlopment\n",
      "community is very eager to meet Van Rossum\n"
     ]
    }
   ],
   "source": [
    "print(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Van',\n",
       " 'Rossum',\n",
       " 'is',\n",
       " 'Python',\n",
       " 'createor',\n",
       " ',',\n",
       " 'visiting',\n",
       " 'Pune',\n",
       " 'this',\n",
       " 'week',\n",
       " '.',\n",
       " 'The',\n",
       " 'devlopment',\n",
       " 'community',\n",
       " 'is',\n",
       " 'very',\n",
       " 'eager',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'Van',\n",
       " 'Rossum']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Van Rossum',\n",
       " 'is',\n",
       " 'Python',\n",
       " 'createor',\n",
       " ',',\n",
       " 'visiting',\n",
       " 'Pune',\n",
       " 'this',\n",
       " 'week',\n",
       " '.',\n",
       " 'The',\n",
       " 'devlopment',\n",
       " 'community',\n",
       " 'is',\n",
       " 'very',\n",
       " 'eager',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'Van Rossum']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the class \n",
    "from nltk.tokenize import MWETokenizer\n",
    "tk = MWETokenizer(separator = \" \")\n",
    "\n",
    "# add mluti word Expression\n",
    "tk.add_mwe(('Van','Rossum'))\n",
    "\n",
    "# Tokenize the data\n",
    "tk.tokenize(word_tokenize(sent1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Tweet Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent  = 'Hello Friends :) ! How are you? Wellcome to the world of Python Programming :D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends :) ! How are you? Wellcome to the world of Python Programming :D\n"
     ]
    }
   ],
   "source": [
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends',\n",
       " ':)',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Wellcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'Python',\n",
       " 'Programming',\n",
       " ':D']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the class \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tk = TweetTokenizer()\n",
    "\n",
    "# Tokenize the data\n",
    "tk.tokenize(sent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends!ðŸ˜€ How are you?ðŸ‘‹\n",
      "WelcomeðŸ™ tothe worldðŸŒŽ of python Programming.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open('mydata.txt')\n",
    "data = f.read()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends',\n",
       " '!',\n",
       " 'ðŸ˜€',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'ðŸ‘‹',\n",
       " 'WelcomeðŸ™',\n",
       " 'tothe',\n",
       " 'worldðŸŒŽ',\n",
       " 'of',\n",
       " 'python',\n",
       " 'Programming',\n",
       " '.']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends',\n",
       " '!',\n",
       " 'ðŸ˜€',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'ðŸ‘‹',\n",
       " 'Welcome',\n",
       " 'ðŸ™',\n",
       " 'tothe',\n",
       " 'world',\n",
       " 'ðŸŒŽ',\n",
       " 'of',\n",
       " 'python',\n",
       " 'Programming',\n",
       " '.']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the class \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tk = TweetTokenizer()\n",
    "\n",
    "# Tokenize the data\n",
    "tk.tokenize(data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "is\n",
      "some\n",
      "text\n",
      "with\n",
      "punctuation\n",
      ">\n",
      "let's\n",
      "tokenize\n",
      "it\n",
      "Is\n",
      "it\n",
      "ok?\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    return re.split(r\"[.,;!\\s]+\", text)\n",
    "\n",
    "text = \"This is some text with punctuation > let's tokenize it. Is it ok?\"\n",
    "\n",
    "tokens = custom_tokenizer(text)\n",
    "\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://mitu.co.in/dataset\n",
    "#Download: student3.tsv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['roll\\tname\\tclass\\tmarks\\tage\\n', '1\\tanil\\tTE\\t56.77\\t22\\n', '2\\tamit\\tTE\\t59.77\\t21\\n', '3\\taniket\\tBE\\t76.88\\t19\\n', '4\\tajinkya\\tTE\\t69.66\\t20\\n', '5\\tasha\\tTE\\t63.28\\t20\\n', '6\\tayesha\\tBE\\t49.55\\t20\\n', '7\\tamar\\tBE\\t65.34\\t19\\n', '8\\tamita\\tBE\\t68.33\\t23\\n', '9\\tamol\\tTE\\t56.75\\t20\\n', '10\\tanmol\\tBE\\t78.66\\t21\\n']\n"
     ]
    }
   ],
   "source": [
    "f = open('student3.tsv')\n",
    "data = f.readlines()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['roll', 'name', 'class', 'marks', 'age\\n']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##try by me \n",
    "# import the class \n",
    "from nltk.tokenize import TabTokenizer\n",
    "tk = TabTokenizer()\n",
    "\n",
    "# Tokenize the data\n",
    "lst = []\n",
    "\n",
    "for line in data:\n",
    "    b = tk.tokenize(line)\n",
    "    lst.append(b)\n",
    "\n",
    "lst.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1', 'anil', 'TE', '56.77', '22\\n'], ['2', 'amit', 'TE', '59.77', '21\\n'], ['3', 'aniket', 'BE', '76.88', '19\\n'], ['4', 'ajinkya', 'TE', '69.66', '20\\n'], ['5', 'asha', 'TE', '63.28', '20\\n'], ['6', 'ayesha', 'BE', '49.55', '20\\n'], ['7', 'amar', 'BE', '65.34', '19\\n'], ['8', 'amita', 'BE', '68.33', '23\\n'], ['9', 'amol', 'TE', '56.75', '20\\n'], ['10', 'anmol', 'BE', '78.66', '21\\n']]\n"
     ]
    }
   ],
   "source": [
    "print(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roll\tname\tclass\tmarks\tage\n",
      "1\tanil\tTE\t56.77\t22\n",
      "2\tamit\tTE\t59.77\t21\n",
      "3\taniket\tBE\t76.88\t19\n",
      "4\tajinkya\tTE\t69.66\t20\n",
      "5\tasha\tTE\t63.28\t20\n",
      "6\tayesha\tBE\t49.55\t20\n",
      "7\tamar\tBE\t65.34\t19\n",
      "8\tamita\tBE\t68.33\t23\n",
      "9\tamol\tTE\t56.75\t20\n",
      "10\tanmol\tBE\t78.66\t21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open('student3.tsv')\n",
    "data = f.read()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['roll', 'name', 'class', 'marks', 'age'],\n",
       " [1, 'anil', 'TE', 56.77, 22],\n",
       " [2, 'amit', 'TE', 59.77, 21],\n",
       " [3, 'aniket', 'BE', 76.88, 19],\n",
       " [4, 'ajinkya', 'TE', 69.66, 20],\n",
       " [5, 'asha', 'TE', 63.28, 20],\n",
       " [6, 'ayesha', 'BE', 49.55, 20],\n",
       " [7, 'amar', 'BE', 65.34, 19],\n",
       " [8, 'amita', 'BE', 68.33, 23],\n",
       " [9, 'amol', 'TE', 56.75, 20],\n",
       " [10, 'anmol', 'BE', 78.66, 21],\n",
       " ['']]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given byu sir\n",
    "newdata = []\n",
    "for x in data.split('\\n'):\n",
    "    inner_list = []\n",
    "    for y in x.split('\\t'):\n",
    "        if y.isdigit():\n",
    "            inner_list.append(int(y))\n",
    "        elif y.find('.') > 0:\n",
    "            inner_list.append(float(y))\n",
    "        else:\n",
    "            inner_list.append(y)\n",
    "\n",
    "    newdata.append(inner_list)\n",
    "newdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['roll', 'name', 'class', 'marks', 'age']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdata.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'anil', 'TE', 56.77, 22],\n",
       " [2, 'amit', 'TE', 59.77, 21],\n",
       " [3, 'aniket', 'BE', 76.88, 19],\n",
       " [4, 'ajinkya', 'TE', 69.66, 20],\n",
       " [5, 'asha', 'TE', 63.28, 20],\n",
       " [6, 'ayesha', 'BE', 49.55, 20],\n",
       " [7, 'amar', 'BE', 65.34, 19],\n",
       " [8, 'amita', 'BE', 68.33, 23],\n",
       " [9, 'amol', 'TE', 56.75, 20],\n",
       " [10, 'anmol', 'BE', 78.66, 21],\n",
       " ['']]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
